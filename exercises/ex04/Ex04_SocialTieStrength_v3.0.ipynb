{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing/Social Gaming - Summer 2021\n",
    "# Exercise Sheet 4 - Social Tie Strength\n",
    "\n",
    "In this exercise, you are going to predict Tie Strength in a social network using the method explained in the paper _E. Gilbert and K. Karahalios: Predicting Tie Strength With Social Media_ [1], of which a short introduction is provided to you in the exercise files. According to Mark Granovetter, the strength of a tie between two persons is a combination of the amount of time, the emotional intensity, the intimacy and the reciprocal services which characterize it. Using variables that describe these categories, we want to find out how much each one of these features contributes in order to predict the strength of ties not previously known.<br>\n",
    "An important prerequisite to this exercise is understanding the basic concept of linear regression models. As mentioned in the lecture, a recommended reading is chapter 3 of _C. Bishop: Pattern Recognition and Machine Learning_ [2], which you can find on [Moodle](https://www.moodle.tum.de/) [3].\n",
    "\n",
    "\n",
    "### Tie Strength Prediction\n",
    "\n",
    "In social network analysis, the Tie Strength between two people measures how strong their relationship is. The paper above describes the procedure of deriving available information (different variables) about a connection between two persons from an online social network and using it in order to discover how close they are. The ultimate goal is to build a model using the given information, finding out which variables account most for the Tie Strength and using that model later on to predict social Tie Strength when only the predictive (or explanatory) variables are available. Before being able to predict anything, we need to find out whether the given variables are suitable for prediction in the first place. This can be done via creating and evaluating a **multiple linear regression model**. 'Multiple' here refers to having more than one predictive variable in an regression model.<br>\n",
    "In the paper mentioned above, 67 variables where used in the linear model to predict the Tie Strength. In our simplified model, we are going to use only 10 predictive variables which are:\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img style=\"margin-left: 5em\" src=\"img/Variables.PNG\" width=\"400\"/>\n",
    "</div>\n",
    "<br>  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We are going to use a simplified form of the paper's linear model:\n",
    "$$y_i = \\alpha + \\beta X_i + \\epsilon_i$$\n",
    "\n",
    "where $y_i$ is the dependent variable (also referred to as target value, which is the Tie Strength in our case) of the $i$-th friend of a person. $X_i$ is the predictive vector, containing the (predictive) variables listed above. $\\alpha$ and $\\beta$ are the model's parameters, where $\\alpha$ is the intercept/bias, $\\beta$ the coefficient vector containing coefficients for each predictive variable, and $\\epsilon$ the prediction error. The regression problem boils down to calculating the model's parameters given a certain ground truth; meaning that for some connections, the Tie Strength has to be already known for building the model. That way, the unknown Tie Strengths can be predicted using the regression model by simply inserting the values into the vector. The coefficients for each predictive variable will show us the importance of the respective variable for the social Tie Strength.\n",
    "\n",
    "### Problem Overview\n",
    "\n",
    "The input to your Python program is a directed social network _SocialGraph.gml_. As the first step, you will visualize the graph with NetworkX to get an overview over the data.\n",
    "\n",
    "In practice, the ground truth (Tie Strength in our case) is usually retrieved by participant's answers to surveys on how strong their relationship is with another person - this is why the graph is directed: two people might have varying views. The ground truth is available in the file. About 70% of the edges have valid values for the `tieStrength` variable, which should be used for training. For about 30% of the edges, the variable is set to -1 (equivalent to unknown). These represent the prediction set for which the Tie Strength should be predicted using the linear regression model later. But first, that model needs to be computed and checked for its goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Imports and Visualization\n",
    "First, needed libraries and the graph's .gml file have to be imported. The social graph is visualized in order to get an idea what the network actually looks like.\n",
    "Inspect the plotted graph. **Describe** shortly, what the graph's visualization is telling you, and if there are any problems with this representation. **Any ideas** on how to improve the visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx, numpy as np, pandas as pd, statsmodels.api as sm, matplotlib.pyplot as plt\n",
    "\n",
    "# read in the structure\n",
    "g = nx.read_gml('SocialGraph.gml', label='id')\n",
    "\n",
    "\n",
    "# formatting the graph and applying spring layout\n",
    "fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "pos=nx.spring_layout(g, k=0.4, iterations=5)\n",
    "\n",
    "visual_style = {\n",
    "    \"node_size\": 300,\n",
    "    \"node_color\": \"#4089EF\",\n",
    "    \"bbox\" : (700,700), \n",
    "    \"with_labels\" : False\n",
    "}\n",
    "\n",
    "nx.draw(g, pos, **visual_style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations and ideas here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Complete and convert the data\n",
    "\n",
    "To further work with our data set, we will now convert it to a [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) [4] dataframe. \n",
    "Some of our predictive variables are not yet computed in the _gml_ file, therefore you have to **calculate the missing variables** from the graph's attributes. You can take a look at the _gml_ file as it is human-readable to see what variables are available for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates the missing values for current edge e of graph g\n",
    "def calculate_missing_variables(g, e):\n",
    "    # the both nodes connected by edge e\n",
    "    first, second = e\n",
    "    # edge data such as firstComm and tieStrength\n",
    "    edge_data = g.get_edge_data(first, second)\n",
    "    \n",
    "    # Source and target nodes for current edge\n",
    "    src = g.nodes[first]\n",
    "    tgt = g.nodes[second]\n",
    "        \n",
    "    # Already existing variables\n",
    "    days_last_comm = edge_data['lastComm']\n",
    "    photos_together = edge_data['photosTogether']\n",
    "    wall_intim_words = edge_data['wallIntimWords']\n",
    "    inbox_intim_words = edge_data['inboxIntimWords']\n",
    "    days_first_comm = edge_data['firstComm']\n",
    "    \n",
    "    # The Ground Truth\n",
    "    tie_strength = edge_data['tieStrength']\n",
    "\n",
    "    \n",
    "    # TODO: Compute the missing values\n",
    "    age_dist = #TODO\n",
    "    edu_diff = #TODO\n",
    "    num_friends = #TODO\n",
    "    friends_num_friends = #TODO\n",
    "    num_mutual_friends = #TODO\n",
    "\n",
    "    \n",
    "    # Create row for dataframe\n",
    "    row = [num_friends, friends_num_friends, days_last_comm, photos_together, wall_intim_words, inbox_intim_words, days_first_comm, num_mutual_friends, age_dist, edu_diff]\n",
    "    row = [int(attr) for attr in row]\n",
    "    row.append(tie_strength) # Appended separately, needs to be float\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "# Training and prediction lists\n",
    "train_list = []\n",
    "pred_list = []\n",
    "cols = ['#Friends', 'Friends\\' #Friends', '#Days Since Last Comm', '#Photos', '#Wall Intimacy Words', '#Inbox Intimacy Words', '#Days Since First Comm','#Mutual Friends', 'Age Dist', 'Educational Diff', 'Tie Strength']\n",
    "\n",
    "# Calculate rows (one for each edge) and add them to tables\n",
    "\n",
    "for e in g.edges:\n",
    "    row = calculate_missing_variables(g, e)\n",
    "    \n",
    "    first, second = e\n",
    "    edge = g.get_edge_data(first, second)\n",
    "    \n",
    "    if edge['tieStrength'] != -1:\n",
    "        train_list.append(row)\n",
    "    else:\n",
    "        pred_list.append(row)\n",
    "        \n",
    "# Create training and prediction tables\n",
    "train_table = pd.DataFrame(train_list, columns=cols)\n",
    "pred_table = pd.DataFrame(pred_list, columns=cols)\n",
    "train_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) The Variance Inflation Factor (VIF)\n",
    "Multiple linear regression can hold some pitfalls if you do not evaluate your data beforehands. Such a pitfall is containing multicollinearity in your predictive variables. \n",
    "\n",
    "Find out and **explain** in your own words what multicollinearity is, why it forms a danger to linear regression models and how the VIF is linked to that. \n",
    "**Create** a temporary dataframe containing only the predictive variables and **add a constant value** to the dataframe for the VIF to produce representative values. Then **compute the VIFs** for them. Statsmodels `variance_inflation_factor()` and `add_constant()` will help you with that. \n",
    "\n",
    "Additionally **explain**: What do the results tell you? Do we have to make any adaptions deriving from them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "\n",
    "# TODO: Creat a dataframe, add a constant & compute VIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your explanaitions here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Log-Transformation\n",
    "In data analysis, transformation is the replacement of a variable by a function of that variable. For example, replacing a variable $x$ by its square root $\\sqrt{x}$ or its logarithm $\\log{x}$. In a stronger sense, it means a replacement that changes the shape of a distribution or relationship. There are many reasons for transformations. The following list are a few of them but it is not comprehensive:\n",
    "\n",
    "1. Convenience\n",
    "2. Reducing skewness\n",
    "3. Equal spreads\n",
    "4. Linear relationships\n",
    "5. Additive relationships\n",
    "\n",
    "If you are looking at just one explanatory variable, points 1, 2 and 3 are relevant. But considering two or more variables, points 4 and 5 are more important. However, transformations that achieve 4 and 5 very often fulfill 2 and 3 as well.\n",
    "\n",
    "In our case, a logarithmic transformation on the predictive variables will be used. It will help us to minimize the impact of any non-linearity on our model.\n",
    "\n",
    "With the help of numpy's `log` function, **apply the log-transformation on each feature vector for the training table (but not the Tie Strength)**. Pay attention to the fact that $\\log{0}$ is not defined, a small number (e.g. 0.000001) should be added before the transformation! Again, output the first ten entries of your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply log transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: The Regression Model\n",
    "\n",
    "### a) Building the model\n",
    "**1.**\n",
    "Finally, the regression can be applied on the dataframe. For this purpose, **split** the dataframe into `y`: the target variable and `X`: the predictive variables. As you have read above, our model contains a bias/intercept named $\\alpha$. This will be realized in the model by adding a constant (1.0), that gets multiplied with its own coefficient and therewith forms the intercept. It represents the target value when all explanatory variables are zero. Once again `add_constant(X)` will be of use.\n",
    "\n",
    "**Split** the dataframe, **add** the constant and then **apply** a multiple linear regression on the training table, the statsmodels functions `OLS()` and `fit()` will help you with that. Output the summary with `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add constant & build the regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.**\n",
    "As you can see the model's summary provides us with a multitude of informations about its performance. Now we need to evaluate our model based on these values. Find out what the meaning of the following statistics are: `R-squared`, `Adj. R-squared`, `Prob (F-statistic)`, the predicitve variables' significances `P>|t|`. [This site](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/multiple-regression/interpret-the-results/key-results/) [5] does a good job explaining them intuitively.\n",
    "\n",
    "**Evaluate** our model's performance by giving a short comment on the obtained values for them. Don't write more than 5 sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your evaluation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.**\n",
    "Now additionally **compare** the obtained coefficients `coef` for our predictive variables to the findings of the paper referenced in [1]. Wich kind of variables (Intimacy, Duration, Structural, Social distance) have the most influence on the Tie Strength according to our regression? You can also comment on specific predicitive variables' values. Keep in mind that the paper's coefficients are already standardized regarding the variabe's values, while ours do not yet compensate for them. Don't write more than 5 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) OPTIONAL: Goodness of Fit\n",
    "After you have now analyzed some of the statistics of our model, there are some additional methods of analyzing the Goodness of Fit of our model. There are several methods to evaluate the Goodness of Fit of a regression. In this exercise, you will work with two of them: the Q-Q Plot and the Residual Plot.\n",
    "\n",
    "**1.: Q-Q Plot**\n",
    "\n",
    "Create a Q-Q Plot and evaluate what the result means for your fit. Plot the model's residuals on one axis and the normal distribution on the other axis, `scipy.stats` will provide it to you. What does the result tell you regarding your fit? Don't write more than 4 sentences.\n",
    "\n",
    "**Hint:** Statsmodles offers a function for Q-Q Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# TODO: Create the QQ-Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your interpretation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.: Residual Plot**\n",
    "\n",
    "Now evaluate your fit by plotting the residuals with matplotlib. The plot should show the standardized residuals for each entry. What does the result tell you regarding your fit? Don't write more than 4 sentences.\n",
    "\n",
    "**Hint:** The standardized residuals can be accessed via `model.resid_pearson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create the Residual-Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your interpretation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Prediction of Tie Strengths\n",
    "As a last step, the missing tie string values (marked with -1) for the entries contained in the `pred_table` should be predicted using the before computed regression model. **Use the regression model to predict the missing Tie Strength values.** Statsmodels will be of help with that. **Remember** that we transformed the training data with a log-transform and added an intercept, so this needs to be done here as well. Output the first ten entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Perform log transformation, add constant & predict the Tie Strengths\n",
    "\n",
    "\n",
    "\n",
    "# An example for queries:\n",
    "# pred_table[pred_table['Tie Strength'] > 0.7].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are the predictions in line with the observations above? Pick a few entries to back up your observations.** If you would like to talk about other than the first ten entries, you can query a pandas dataframe similar to SQL. More information on how to do this is available in the [pandas documenation](https://pandas.pydata.org/pandas-docs/version/0.19.2/comparison_with_sql.html) [4].\n",
    "\n",
    "As you might discover, there are some Tie Strength values slightly below zero. Can you **explain** that behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observation and explanation here!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] E. Gilbert and K. Karahalios: _Predicting Tie Strength With Social Media_. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2009.\n",
    "<br>[2] C. Bishop: _Pattern Recognition and Machine Learning_. 2006.\n",
    "<br>[3] https://www.moodle.tum.de/\n",
    "<br>[4] https://pandas.pydata.org/docs/user_guide/index.html\n",
    "<br>[5] https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/multiple-regression/interpret-the-results/key-results/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
